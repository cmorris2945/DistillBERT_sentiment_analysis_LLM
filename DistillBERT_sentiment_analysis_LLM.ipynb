{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVKUjXmDUTCzFZRA/+JJtb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cmorris2945/DistillBERT_sentiment_analysis_LLM/blob/main/DistillBERT_sentiment_analysis_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aInvY_I0OuiH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "## have to first install some modules that is not indegenous to Jupyter....\n",
        "\n",
        "!pip install transformers datasets evaluate\n",
        "\n",
        "!pip install langchain\n"
      ],
      "metadata": {
        "id": "gHIqvdHlOvyY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40a9d65e-e06f-4bdd-b23f-9902050eb32e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.28)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.96)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## These are the modules that have the methods, classes and funcations\n",
        "## to get data, tokenize, train and so forth also logging into the Hugging Face Hub.\n",
        "# This is super important because it lets us access models and datasets directly from their library....\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from huggingface_hub import login\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "# This is the login method and the token parameter it takes....\n",
        "login(token=\"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\")"
      ],
      "metadata": {
        "id": "wBWkLvXAOv1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d8b4a6a-2e13-4629-d78c-69f24f68dca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now here I am defining a custom sequential chain class.\n",
        "# This class will help me run the data through a series of processing steps.\n",
        "class SequentialChain:\n",
        "    def __init__(self, chains):\n",
        "        # Initialize the chain with a list of processing steps.\n",
        "        self.chains = chains\n",
        "\n",
        "    def run(self):\n",
        "        data = None\n",
        "        # Loop through each step in the chain and process the data.\n",
        "        for chain in self.chains:\n",
        "            data = chain[\"function\"](data)\n",
        "        # Return the final processed data.\n",
        "        return data\n",
        "\n",
        "\n",
        "\n",
        "# Now, I'll create a LangChain component for loading our dataset.\n",
        "class DataLoader:\n",
        "    def __init__(self, dataset_name):\n",
        "        # Initialize the DataLoader with the name of the dataset we want to load.\n",
        "        self.dataset_name = dataset_name\n",
        "\n",
        "    def __call__(self, _):\n",
        "        # Load the dataset using the load_dataset function from the datasets library.\n",
        "        return load_dataset(self.dataset_name)\n",
        "\n",
        "# Create an instance of the DataLoader for the \"Yelp\" Review dataset. (I chose the Yelp review dataset)\n",
        "data_loader = DataLoader(\"yelp_review_full\")\n",
        "\n",
        "\n",
        "\n",
        "# No I need a function to tokenize our text data so the model can understand it.\n",
        "# This function will convert the text into tokens, which are the basic units the model works with.\n",
        "def tokenize_function(examples, tokenizer):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oDYZ7ISYoe5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5stI37DAofN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LangChain components for tokenization\n",
        "class Tokenizer:\n",
        "    def __init__(self, tokenizer_name):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "\n",
        "    def __call__(self, dataset):\n",
        "        return dataset.map(lambda examples: tokenize_function(examples, self.tokenizer), batched=True)\n",
        "\n",
        "tokenizer = Tokenizer(\"distilbert-base-cased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFP0BKB5ofXx",
        "outputId": "2a15ec6d-806c-43c7-9260-f578dfd02524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the model training class\n",
        "class ModelTrainer:\n",
        "    def __init__(self, model_name, output_dir, num_labels):\n",
        "        # We're loading a pre-trained model for sequence classification.\n",
        "        # Here, I'm using 'distilbert-base-cased' as the base model.\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "        # Set up the training arguments.\n",
        "        # This includes where to save the model, how many epochs to train for,\n",
        "        # batch sizes for training and evaluation, and enabling mixed precision training.\n",
        "        self.training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            num_train_epochs=5,  # Set number of epochs (more epochs can help the model learn better)\n",
        "            per_device_train_batch_size=16,  # Batch size for training\n",
        "            per_device_eval_batch_size=16,  # Batch size for evaluation\n",
        "            fp16=True  # Enable mixed precision training for faster and more efficient training\n",
        "        )\n",
        "\n",
        "        # Load the accuracy metric to evaluate our model.\n",
        "        self.metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "    # Define a function to compute the metrics during evaluation.\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        return self.metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    # Define the call method to run the training process.\n",
        "    def __call__(self, tokenized_datasets):\n",
        "        # Create smaller subsets of the tokenized datasets for quicker training and evaluation.\n",
        "        small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(500))  # Smaller subset for quicker training\n",
        "        small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500))  # Smaller subset for quicker training\n",
        "\n",
        "        # Initialize the Trainer from transformers to handle the training loop.\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=self.training_args,\n",
        "            train_dataset=small_train_dataset,\n",
        "            eval_dataset=small_eval_dataset,\n",
        "            compute_metrics=self.compute_metrics,\n",
        "        )\n",
        "\n",
        "        # Start the training process...\n",
        "        trainer.train()\n",
        "        # Save the trained model to the specified output directory.\n",
        "        trainer.save_model(\"fine_tuned_model\")\n",
        "\n",
        "        # Evaluate the model on the evaluation dataset.\n",
        "        results = trainer.evaluate()\n",
        "        # Print out the evaluation results.\n",
        "        print(results)\n",
        "\n",
        "        # Return the evaluation results.\n",
        "        return results\n",
        "\n",
        "# Create an instance of the ModelTrainer class with the specified model and output directory.\n",
        "model_trainer = ModelTrainer(\"distilbert-base-cased\", \"test_trainer\", num_labels=5)\n",
        "\n",
        "# Create a custom sequential chain.\n",
        "# This chain will run through data loading, tokenization, and model training steps in sequence.\n",
        "chain = SequentialChain(\n",
        "    chains=[\n",
        "        {\"function\": data_loader, \"name\": \"Data Loader\"},\n",
        "        {\"function\": tokenizer, \"name\": \"Tokenizer\"},\n",
        "        {\"function\": model_trainer, \"name\": \"Model Trainer\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Run the chain to execute the entire process from data loading to model training and evaluation.\n",
        "results = chain.run()\n",
        "\n"
      ],
      "metadata": {
        "id": "Z5UbNHogOwAF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "81dfb767-865d-47e6-ab92-aceff6b30743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [160/160 01:05, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.358729</td>\n",
              "      <td>0.376000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.095432</td>\n",
              "      <td>0.518000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.080273</td>\n",
              "      <td>0.554000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.136171</td>\n",
              "      <td>0.564000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.133183</td>\n",
              "      <td>0.560000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32/32 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 1.1331825256347656, 'eval_accuracy': 0.56, 'eval_runtime': 3.2746, 'eval_samples_per_second': 152.692, 'eval_steps_per_second': 9.772, 'epoch': 5.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ETXvw5imOwCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nw8YoHfTnGCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ky7ygN_InGHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AhaIwJLHnGJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary classes from transformers and LangChain.\n",
        "# I am going to use these to load my fine-tuned model, tokenize the input text, and define the inference process.\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# Load the fine-tuned model and tokenizer from the local directory.\n",
        "# This is the model I just trained, and the tokenizer we used for preprocessing.\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"fine_tuned_model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
        "\n",
        "# Define the inference class.\n",
        "# This class will handle the process of making predictions with our fine-tuned model.\n",
        "class Inference:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        # Initialize with the model and tokenizer.\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    # Define the call method to make a prediction.\n",
        "    def __call__(self, text):\n",
        "        # Tokenize the input text, padding and truncating as necessary.\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        # Use the model to get predictions.\n",
        "        outputs = self.model(**inputs)\n",
        "        # The model outputs logits, which we need to convert to predictions.\n",
        "        predictions = np.argmax(outputs.logits.detach().numpy(), axis=-1)\n",
        "        # Return the prediction.\n",
        "        return predictions[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "JMi5s_feOwEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the inference step by creating an instance of the Inference class.\n",
        "inference = Inference(model, tokenizer)\n",
        "\n",
        "# Now, here, I'm defining a LangChain prompt template for user input.\n",
        "# This template will help format the input text for the model...\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Classify the sentiment of this review: {text}\",\n",
        ")\n",
        "\n",
        "# Create a custom sequence for inference.\n",
        "# This class will handle the process of formatting the input and getting predictions from the model.\n",
        "class CustomChain:\n",
        "    def __init__(self, template, llm):\n",
        "        # Initialize with the template and the language model (llm).\n",
        "        self.template = template\n",
        "        self.llm = llm\n",
        "\n",
        "    # Define the run method to process the input data and get predictions.\n",
        "    def run(self, input_data):\n",
        "        # Format the input text using the template.\n",
        "        prompt = self.template.format(text=input_data[\"text\"])\n",
        "        # Get the prediction from the language model.\n",
        "        result = self.llm(prompt)\n",
        "        # Return the prediction result.\n",
        "        return result\n",
        "\n",
        "# Initialize the custom chain by creating an instance of the CustomChain class.\n",
        "custom_chain = CustomChain(template=template, llm=inference)\n",
        "\n",
        "# Now I test the model with some example inferences here.\n",
        "# Here are some sample texts that we'll use to see how our model performs...\n",
        "texts = [\n",
        "    \"This is a wonderful place to eat!\",\n",
        "    \"The food was terrible and the service was worse.\",\n",
        "    \"It was okay, nothing special.\",\n",
        "    \"I had a great time and the staff was very friendly.\",\n",
        "    \"I would not recommend this place to anyone.\"\n",
        "]\n",
        "\n",
        "# Loop through each text, run the custom chain to get predictions, and print the results.\n",
        "for text in texts:\n",
        "    result = custom_chain.run({\"text\": text})\n",
        "    print(f\"Review: {text}\")\n",
        "    print(f\"Predicted sentiment: {result}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKgFtb3cqAQZ",
        "outputId": "9fc69f8a-acc5-4785-fbc3-493aab7ea0cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: This is a wonderful place to eat!\n",
            "Predicted sentiment: 4\n",
            "\n",
            "Review: The food was terrible and the service was worse.\n",
            "Predicted sentiment: 1\n",
            "\n",
            "Review: It was okay, nothing special.\n",
            "Predicted sentiment: 1\n",
            "\n",
            "Review: I had a great time and the staff was very friendly.\n",
            "Predicted sentiment: 4\n",
            "\n",
            "Review: I would not recommend this place to anyone.\n",
            "Predicted sentiment: 0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XA7CsnAOOwGf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}